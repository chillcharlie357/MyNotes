annotation-target::![[TACTIC.pdf]]

>%%
>```annotation-json
>{"created":"2023-04-21T03:02:01.463Z","updated":"2023-04-21T03:02:01.463Z","document":{"title":"TACTIC.pdf","link":[{"href":"urn:x-pdf:f45fa5799422dd0e1468c9643d5de3e4"},{"href":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf"}],"documentFingerprint":"f45fa5799422dd0e1468c9643d5de3e4"},"uri":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf","target":[{"source":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf","selector":[{"type":"TextPositionSelector","start":367,"end":507},{"type":"TextQuoteSelector","exact":"Existing ap-proaches often just focus on finding erroneousbehaviours and have not thoroughly studied theimpact of environmental conditions. ","prefix":"stems is of growing importance. ","suffix":"In this pa-per, we propose to te"}]}]}
>```
>%%
>*%%PREFIX%%stems is of growing importance.%%HIGHLIGHT%% ==Existing ap-proaches often just focus on finding erroneousbehaviours and have not thoroughly studied theimpact of environmental conditions.== %%POSTFIX%%In this pa-per, we propose to te*
>%%LINK%%[[#^49s6uu4sa9e|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^49s6uu4sa9e



>%%
>```annotation-json
>{"created":"2023-04-21T03:02:51.923Z","updated":"2023-04-21T03:02:51.923Z","document":{"title":"TACTIC.pdf","link":[{"href":"urn:x-pdf:f45fa5799422dd0e1468c9643d5de3e4"},{"href":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf"}],"documentFingerprint":"f45fa5799422dd0e1468c9643d5de3e4"},"uri":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf","target":[{"source":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf","selector":[{"type":"TextPositionSelector","start":872,"end":967},{"type":"TextQuoteSelector","exact":"o identify critical envi-ronmental conditions generated by an image-to-image translation model.","prefix":"employsthe search-based method t","suffix":" Large-scale experimentsshow tha"}]}]}
>```
>%%
>*%%PREFIX%%employsthe search-based method t%%HIGHLIGHT%% ==o identify critical envi-ronmental conditions generated by an image-to-image translation model.== %%POSTFIX%%Large-scale experimentsshow tha*
>%%LINK%%[[#^somhfnrm69a|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^somhfnrm69a


>%%
>```annotation-json
>{"created":"2023-04-21T03:07:39.596Z","updated":"2023-04-21T03:07:39.596Z","document":{"title":"TACTIC.pdf","link":[{"href":"urn:x-pdf:f45fa5799422dd0e1468c9643d5de3e4"},{"href":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf"}],"documentFingerprint":"f45fa5799422dd0e1468c9643d5de3e4"},"uri":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf","target":[{"source":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf","selector":[{"type":"TextPositionSelector","start":5656,"end":5808},{"type":"TextQuoteSelector","exact":"TACTIC employsthe Multimodal Unsupervised Image-to-Image Translation(MUNIT) (Huang et al., 2018) to deep-learn the features ofenvironmental conditions f","prefix":"nstead of using configurations, ","suffix":"rom a set of sample scenes be-lo"}]}]}
>```
>%%
>*%%PREFIX%%nstead of using configurations,%%HIGHLIGHT%% ==TACTIC employsthe Multimodal Unsupervised Image-to-Image Translation(MUNIT) (Huang et al., 2018) to deep-learn the features ofenvironmental conditions f== %%POSTFIX%%rom a set of sample scenes be-lo*
>%%LINK%%[[#^bm3of8nbayv|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^bm3of8nbayv


>%%
>```annotation-json
>{"created":"2023-04-21T03:08:12.085Z","text":"天气、光线等","updated":"2023-04-21T03:08:12.085Z","document":{"title":"TACTIC.pdf","link":[{"href":"urn:x-pdf:f45fa5799422dd0e1468c9643d5de3e4"},{"href":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf"}],"documentFingerprint":"f45fa5799422dd0e1468c9643d5de3e4"},"uri":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf","target":[{"source":"vault:/%E5%A4%A7%E5%88%9B/paper/assets/TACTIC.pdf","selector":[{"type":"TextPositionSelector","start":5947,"end":6096},{"type":"TextQuoteSelector","exact":"A style,which is a vector in the style space corresponding to an envi-ronmental condition, contains extremely rich and complexenvironmental features.","prefix":"ector space called style space. ","suffix":" To identify the styles correspo"}]}]}
>```
>%%
>*%%PREFIX%%ector space called style space.%%HIGHLIGHT%% ==A style,which is a vector in the style space corresponding to an envi-ronmental condition, contains extremely rich and complexenvironmental features.== %%POSTFIX%%To identify the styles correspo*
>%%LINK%%[[#^h7g4fww00v|show annotation]]
>%%COMMENT%%
>天气、光线等
>%%TAGS%%
>
^h7g4fww00v
