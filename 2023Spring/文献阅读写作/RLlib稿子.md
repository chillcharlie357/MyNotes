# 背景

前面我们介绍了ray对强化学习不同部分的优化，而RLlib提出了更加整体的加速方案。

传统的强化学习算法设计复杂，大多数是完全分布式，可扩展性和代码复用性较差。

而RLlib是一个**基于Ray**分布式计算框架的开源强化学习库，它提供了一组高级接口和算法，用于实现和测试各种强化学习技术。

# 分层控制模型

区别于传统的完全分布式模型，RLlib使用分层控制模型。

传统的完全分布式系统，在调度的复杂性、数据一致性和网络通信都给分布式系统的设计带来极大的困难。而使用逻辑中控和分层控制就一定程度上解决这个问题。

1. RLlib利用Ray的分布式计算功能，实现了**高效的分布式训练**，包括任务并行、数据并行和深度学习模型并行。这使得RLlib能够处理大规模的强化学习工作负载。
2. RLlib通过组合和重用现有模块与算法实现来构建可拓展的强化学习算法。使用分层控制，组件可以保持不变，并且可以作为远程任务简单调用。
3. 封装了多种RL训练算法，由于开放的体系结构，也允许继续扩展。

# 调度策略

基于**逻辑集中式程序控制**（逻辑中控）和**并行封装**的原则。

- 逻辑中控(Logically centralized control)：有中心化的控制器
	- 管理多个智能体（Agent），数据传输和资源调度
	- 把分布式计算任务的各个子任务分配到不同节点上，并行执行提高计算效率，协调子任务之间的通信

- 并行封装(parallelism encapsulation)
	- 把并行计算的接口和实现分离，隐藏复杂的细节，使得用户可以更方便地进行并行计算。
	- 封装后减少耦合，更好地代码复用。
	- 简洁的完成强化学习算法的选择和更改


RLlib采用自顶向下分层控制的分布式强化学习算法，从而更好地采用并行计算资源调度来完成这些计算任务。
可以简洁地选择RL中算法和算法优化的各种选择（同步异步、CPU和GPU、全局规约和参数服务器）。



