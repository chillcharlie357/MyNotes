---
aliases: 
tags: 
categories:
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 2-无监督学习
date:  Tuesday,September 12th 2023
modified:  Tuesday,September 19th 2023
---

# 1. 聚类学习

## 1.1. 相关概念

- 聚类：数据对象的集合
	- 同一个类中，数据对象是相似的
	- 反之，不同类不相似
- 聚类算法
	- 根据给定的标准，加数据集划分成几个聚类

- 划分标准
	- 特征向量看成特征空间的点，分类依据为点之间的距离（相似度）

- 类特征
	- 聚类学习中的类不是是先给定，而是根据数据的相似度和距离划分（<font color="#ff0000">无监督学习</font>）
	- 聚类的数目和结构没有事先给定

- 聚类的目的
	- 潜在的自然分组结构
	- 感兴趣的关系

- 聚类的关键
	- 特征的设计和选择
	- 距离函数

- 有效性
	- 聚类分析方法是否有效与<font color="#ff0000">数据分布</font>有很大关系
	- <font color="#ff0000">特征选择</font>很重要，外在表现为数据分布是否可分

# 2. 距离度量

目的：度量同类样本间的<font color="#ff0000">相似度</font>和不同样本间的<font color="#ff0000">差异性</font>

## 2.1. 度量函数和度量空间

数学定义；->并不是所有函数都是度量函数

1. 非负性
2. 唯一性
3. 对称性
4. 三角不等式

[[2023Fall/机器学习/0-introduction#距离度量函数|度量函数]]

欧氏距离：两个向量的二范数  
曼哈顿距离：一范数  
切比雪夫：无穷范数

MInjowski distance

# 3. 聚类准则

定义例子：设集合S中任意元素$x_i$和$x_j$间的距离有 $d(x_i,x_{j)\le}h$ ，其中h为给定的阈值，称S对于阈值h组成一类

## 3.1. 试探方法

凭直觉或经验，对实际问题定义一种距离度量的<font color="#ff0000">阈值</font>，然后按<font color="#ff0000">最邻近规则</font>指定某些样本属于某个聚类类别

## 3.2. 聚类准则函数方法

一种定义：

$J=\sum\limits_{j=1}^{c}\sum\limits_{x\in S_j}|x-m_j|^2$

# 4. 聚类方法

- 基于试探的聚类搜索算法
- 系统聚类法
- 动态聚类法

## 4.1. 基于试探的聚类搜索算法

按最近邻规则的简单试探法

基于最大最小距离的试探法

## 4.2. 系统聚类法

将数据样本<font color="#ff0000">按照距离准则逐步分类</font>，类别分别由多到少，直到获得合适的分类要求为止。

### 4.2.1. 算法流程

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F09%2F19%2Fd69fb928b621e491fd856969bef3b2d1_20230919184351.png)

### 4.2.2. 距离准则函数

计算**类与类**之间的距离。

- 主要计算准则
	- 最短距离 （两个集合所有距离最小值）
	- 最长距离（两个集合所有距离最大值）
	- 类平均距离（所有距离平均值）

## 动态聚类法

### 基本思想

1. 首先选择若干个样本点作为聚类中心，再按照某种准则使得样本点向各中心聚集，从而获得初始聚类；
2. 然后判断初始分类是否合理，不合理则修改；
3. 如此反复修改聚类

### K-means算法

算法过程：

1. 选择一个聚类数量k
2. 初始化聚类中心$u_1,\dots,u_k$
   - 随机选择k个样本点，设置这些样本点为中心
3. 对每个样本点，计算样本点到k个聚类中心的距离（某种距离度量方法），将样本点**分配到距离它最近的聚类中心的聚类**
4. **重新计算聚类中心**，聚类中心为属于这一聚类的所有样本的均值
5. 如果没有发生**样本所属的聚类发生变化**的情况，则退出；否则，返回step3继续。

- 影响因素
	- k值
	- 初始聚类中心
	- 样本的几何分布

实际应用中，需要试探不同的k值和选择不同的聚类中心的起始值。  
每次运行结果可能完全不一样，可以固定种子把随机数固定下来。

k-means算法比较适用于分类数目已知的情况

### k-means++算法

基本思想：K个初始聚类中心相互分得越开越好（**不再随机选择初始的k个聚类中心**）

算法过程：

1. 从数据集中随机选取一个样本作为初始聚类中心$c_1$
2. 首先计算每个样本与当前已有的聚类中心之间的最短距离（即与最近的一个聚类中心的距离）$D(x)$；接着计算每个样本被选为下一个聚类中心的概率为$\frac{D(x)^2}{\sum\limits_{s\in X}D(x)^2}$。最后，按照轮盘发选择出**下一个聚类中心**
3. 重复第2步知道选择出$K$个聚类中心
4. 之后步骤与K-means算法中的2-4步相同

### ISODATA算法

有分裂/合并操作

**算法过程**：

1. 从数据集随机选取$K_0$个样本作为初始聚类中心$C=\{c_1,c_2,...,c_{K_0}\}$
2. 对每个样本$x_i$，计算它到$K_0$个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中
3. 判断上述每个类中的元素数目是否少于$N_{min}$。如果小于，则需要丢弃该类，令$K=K-1$，并把该类的样本重新分配到剩下类中距离最小的类
4. 针对每个$c_i$重新计算聚类中心
5. 如果$K\le \frac{K_0}{2}$，说明当前类别数太少，分裂
6. 如果$K\ge \frac{K_0}{2}$，说明当前类别数太少，合并
7. 如果达到最大迭代次数则终止，否则回到2

**合并**：

1. 计算当前所有类别的聚类中心两两之间的距离，用矩阵D表示，其中$D(i,i)=0$
2. 对于$D(i,j)\lt d_{min}(i \ne j)$的两个类别需要进行的合并操作，变成一个新的类，该类的聚类中心位置为:
	- $m_{new}=\frac{1}{n_i+n_j}(n_{i}m_{i}+n_{j}m_{j})$
	- 两个类别加权求和

两个类别允许最小距$d_{min}$：是否进行合并的阈值

**分类**：

1. 计算每个类别下所有样本在每个维度下的方差
2. 针对每个类别的所有方差挑出最大的方差$\delta_{max}$  
3. 如果某个类别$\delta_{max}\lt Sigma$并且岩本数量$n_{i}\le 2n_{min}$则可以分裂
4. 满足第3步条件的类分成两个子类并令$K=K+1$
	- 新聚类的中心位置：$m_i^{(+)}=m_i+\delta_{max},m_i^{(-)}=m_i-\delta_{max}$  
最大方差Sigma：衡量样本分散程度

## 比较

1. K-means通常适用于类别数目已知的聚类，ISODATA更灵活
2. ISODATA算法与K-means算法相似，聚类中心都是样本均值的迭代运算决定的
3. 都需要合理选择超参数

# 聚类评价

考虑从一下几个指标评价聚类效果：
1. 聚类中心之间的距离
2. 聚类域的样本数目
3. 聚类域内的样本距离方差

紧密度：$\overline{CP_i}$每个簇内部的紧密程度

间隔度：$\overline{SP}$簇间是否分散

DBI：max含义->看最差的情况


DVI


- 标签已知时的评价标准：
	- P61