---
aliases: 
tags:
  - 2023_Fall_机器学习
  - 课程
categories: 2023_Fall_机器学习
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 5-概率与学习
date: 2023-10-24 18:32
modified: 2023-12-29 10:15
---

# 1. 符号与术语

- 标量scalar: $x \in R$
- 向量vector: $\textbf{x} \in R^{d}, \textbf{x} = \{x_1,x_2,...,x_n\}^T$
- 矩阵matrix:$\textbf{X} \in R^{m \times n}, \textbf{X}$, 2维
- 张量tensor：泛化的实数组成的n维数组

工具书:[[matrixcookbook.pdf]]

## 1.1. 带约束的优化问题

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2F95e0a2533192aa8cd4c8c1cc82c74255_20231024185613.png)

## 1.2. 不带约束的优化问题

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2F95e0a2533192aa8cd4c8c1cc82c74255_20231024185613.png)

## 1.3. 凹凸性

### 1.3.1. 凸函数 convex

$\textbf{X}$是一个凸集合,$f:\textbf{X}\rightarrow \mathbb{R}$表示定义在$\textbf{X}$上的一个函数:

$$
\forall x_1,x_{2}\in \textbf{X},\forall t\in [0,1]:
f(tx_{1}+(1-t)x_{2)}\le tf(x_{1)} + (1-t)f(x_2)
$$

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2F3ce55de7ee7aef15b940dc74ce09ac41_20231024190228.png)

### 1.3.2. 凹函数 concave

$\textbf{X}$是一个凸集合,$f:\textbf{X}\rightarrow \mathbb{R}$表示定义在$\textbf{X}$上的一个函数:

$$
\forall x_1,x_{2}\in \textbf{X},\forall t\in [0,1]:
f(tx_{1}+(1-t)x_{2)}\ge tf(x_{1)} + (1-t)f(x_2)
$$

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2F72b02c5828ddc28e201634b1d9b74902_20231024190236.png)

### 1.3.3. 判断凹凸性

- 二阶导数

$$
x\in dom \, f, x\in \mathbb{R}, 如果 \, f^{''}(x)\ge 0, f(x)是凸函数
$$

- 半正定矩阵

$$
x\in dom \, f, x\in \mathbb{R}^d, 如果 Hessian矩阵半正定, f(x)是凸函数
$$

### 1.3.4. 常见例子

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2F934813d4b21532f78dae45c6775c3350_20231024191325.png)

## 1.4. 随机变量的期望

概率密度函数PDF: 连续  
概率质量函数PMF: 离散

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2Fb1d65cebc07f35fa8aff09452693e996_20231024192035.png)

### 1.4.1. Jensen's inequality

- 如果$X$是随机变量,且$f(X)$是凸函数,则$E(f(X)) \ge f(E[X])$
- 如果$X$是随机变量,且$f(X)$是凹函数,则$E(f(X)) \le f(E[X])$

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2F11e7262a24e4fc310b8716b8dc40a0c0_20231024191903.png)

## 1.5. 高斯分布/正态分布

### 1.5.1. 单变量高斯分布

#TODO

### 1.5.2. 多变量高斯分布

- 若d维随机变量$\mathbf{X}=(X_1,...X_d)^T$服从高斯分布:

$$
\mathbf{X} \sim N(\mathbf{\mu},\Sigma)
$$

$\mu \in \mathbb{R}^d$为均值向量  
$\Sigma \in \mathbb{R}^{d\times d}$为协方差矩阵

- 概率密度函数PDF

#TODO

# 2. 高斯混合模型(Gaussian Mixture Model,GMM)

## 2.1. 概率密度函数

$$
p(x)=\sum\limits_{i=1}^{N} \alpha_{i}N(\mathbf{x};\mathbf{\mu_i},\Sigma_i)
$$

- 随机变量$\mathbf{X}\in \mathbb{R}^d$
- $N$表示有$N$个高斯分布组成
- $\forall i ,\, \alpha_{i}\ge 0,\,  \sum\limits_{i=1}^{N}\alpha_i=1$

模型**参数**: $\theta = \{\mathbf{x};\mathbf{\mu_i},\Sigma_i\}_{i=1}^N$, 可以在模型的训练过程中确定

## 2.2. 将GMM看成图模型

- 假设随机变量$Z\in \{1,2,..,N\}$符合多项式离散分布
- $Z$取值为i的概率$Pr(Z=i)=\alpha_i$

- two-step sampling
	1. 从$Z$中采样,得到一个值$i$,其中($1 \le i \le N$). 先选择一个高斯分量
	2. 从第i个高斯分量$N(\mu_i,\Sigma_i)$得采样$x$

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2F7960bc64dbdc9dfeb58d76e59e449dcc_20231024200435.png)

观测变量由**隐藏变量**决定,$z_i$与$x_i$一一对应

One-hot: 只有一个位置为1的vector

$z_i$代表$x_i$属于哪个高斯分布,**因为一个具体样本点不可能同时来自多个分布**, 只是多个分布的数据混合之后分布表示更复杂

定义了隐藏变量$Z$后,问题变成了如何从$Z$和可观测的变量$X$估计$\theta$

### 2.2.1. 例子

特殊情况,假设$Z$已知,如何估计参数$\theta$?

1. 找到所有从第i个高斯分量得到的采样,构成子集$X_i=\{x_j|z_{j}= i, 1 \le j \le M\}$
2. 统计和计算每个高斯分量的参数

$$
\hat{\alpha_i}=\frac{|X_i|}{M}
$$

#TODO 

# 3. 最大似然估计(Maximum likelihood estimation, MLE)

## 3.1. 定义

- 定义: MLE是通过最大化一个似然函数来估计一个概率分布的参数,使得在假定的统计及模型下,观测数据最有可能出现
	- 数据已经有了,找到一个最合适的概率模型

- 似然函数 $L(\theta | X) = p(X| \theta)$
	- $\theta$固定(数据分布假设固定), $p(X|\theta)$看作是$X$的函数,即为概率密度函数PDF
	- $X$固定(观测数据固定), $L(\theta | X)$看作是$\theta$的函数, 即为似然函数

- i.i.d: 独立同分布,从同一个数据分布采样,但是样本点相互独立
	- 联合概率可以直接使用乘积表示

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2F18763b2e50b2dd92438531ecff782be5_20231024201856.png)

为了方便计算取对数, 称为**对数似然函数**

## 3.2. 以单高斯模型为例

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2Fb33f6163e1f39d01e34969dc4177aff3_20231024203802.png)

# 4. 期望最大化算法 (Expectation-Maximization algorithm,EM)

解决包含两个未知变量的优化问题

## 4.1. 核心思想

EM算法是一个迭代的方法, 采用最大似然估计MLE对统计模型中的参数进行估计, 特别是针对包含无法观测变量的模型.

通常引入隐含变量后会有两个参数, EM算法首先会固定其中的第一个参数, 然后利用MLE计算第二个参数; 在固定第二个参数, 用MLE估计第一个参数;依次迭代.

左右互博.

## 4.2. EM算法

E-Step

M-Step

Repeat

## 4.3. EM优化分析

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F24%2F3fff23509bd81cdffb3210268a3cd52b_20231024204455.png)

Jensen不等式得到下界
