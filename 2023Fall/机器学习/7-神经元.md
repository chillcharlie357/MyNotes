
# Hebb法则

连接强度的调整值与输入输出的乘积成正比，经常出现的模式增强神经元的连接


# MP神经元模型

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F11%2F07%2Ffabc679558d840c6d0dd65a0756fd57c_20231107193143.png)

输入：X
权重：W
输出：激活函数$f(net)=f(\sum\limits(w_{i}\times x_{i}))$

偏置单元：$x_{0}w_{0}$

单个神经元实现and/or，但不能实现xor

# 激活函数

单位跃阶函数

- Sigmoid函数：$f(x)= \frac{1}{1+e^{-x}}$
	- 容易造成梯度消失，无法扩展模型深度
	- 导数为其本身的函数
	- 饱和激活函数

ReLU、Leaky ReLU

# 感知机

早期的前馈式人工神经网络

