---
aliases: 
tags: 
categories:
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 6-支持向量机
date:  Tuesday,October 31st 2023
modified:  Tuesday,October 31st 2023
---

# 回顾

## 统计学角度看机器学习

机器学习目的得到映射: $x\rightarrow y$

- 类先验概率： $p(y=i)$
- 样本先验概率：$p(x)$
- 类的条件概率：$p(x|y=i)$
	- 最大似然估计
- 后验概率：$p(y=i|x)$
	- 给出样本$x$，得到类$y=i$的后验概率，可以判定$x$属于后验概率最高的类

## 统计机器学习方法

### 从概率框架

- 生成式模型 Generative models
	- 估计$p(x|y=i)$和$p(y=i)$，然后贝叶斯定理求$p(y=i|x)$
- 判别式模型 Discriminative models
	- 直接估计$p(y=i|x)$
	- 判别函数 Discriminant fucntion：<font color="#ff0000">不假设概率模型</font>，直接求一个把各类分开的边界

# 感知机

线性超平面：$\mathbf{w}^T\mathbf{x}+b=0$在特征空间可以划分类别

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F31%2Fa86d20b30876e5ffe1688c966af8ed31_20231031185339.png)

# 线性支持向量机

## 间隔和支持向量机

- 间隔:一个点对应间隔(margin)式其到分界超平面的垂直距离
- SVM最大化所有训练样本的最小间隔
- 具有最佳间隔的点称为**支持向量(support vectors)**

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F31%2Fe27762170f979a2c9ea1c836fd8ecb0e_20231031190638.png)



## 计算Margin

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F31%2Fc8b73d3ef4d015deee061644405f9d3c_20231031191039.png)
超平面为$f(x) = w^{T}x+b=  0$

- $x$的投影点为$x_{\bot}$, $x-x_{\bot}$为距离向量
	- 其方向与$\mathbf{w}$相同,为$\frac{w}{||w||}$
	- margin为其大小$r$的**绝对值**
- $x=x_{\bot}+r\frac{w}{||w||}$
	- $w^{Tx}+b = w^{Tx_{\bot}}+ b + r\frac{w}{||w||}$
	- $f(x) = f(x_{\bot}) + r||w||$, 其中$f(x_{\bot})=0$
	- $x$的**margin**为 $r = \frac{f(x)}{||w||}$



## 分类与评价

- 如何分类
	- $f(x)\gt 0$分为正类,$f(x) \lt 0$分为负类

- 对于任何一个样例,如何判断预测对错? ($y_{i}= \{-1,1\}$)
	- $y_if(x_{i)}\gt 0$分类正确,$y_if(x_{i)}\lt 0$分类错误
	- 如果**我们假设能完全分开**,并且$|y_i|=1$,那么$y_if(x_i)=|f(x_i|$


## SVM形式化描述

- SVM的问题:最大化所有训练样本的最小间隔

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F31%2Feb795195f6ee8b8cce2972bf9ad04e42_20231031193958.png)

- 对$w$没有限制,要求最大化最小间隔,难以优化
- 只需要$yf(x) \gt 0$即可判断是否正确分类
	- <font color="#ff0000">只需要方向,不需要大小</font>
	- 如果$(w,b)$变为$(cw,cb)$, 预测和间隔不会变

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F31%2Fb197ef7da56844f6a51002f71f81777c_20231031195417.png)


## 拉格朗日乘子法

把约束条件带到放到目标函数里

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F31%2Fb9c2e58a94b54c643215610c8407f32f_20231031201628.png)

## KKT条件

**互补松弛性质**: 存在拉格朗日乘子$\times$后面一项为0时, 优化函数和原始优化函数一致

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F31%2F5f85110d88aa766f1e1a2306fd7d93d3_20231031201638.png)


## Soft margin

允许少量点margin比1小

$y_i(w^{T}x_{i}+b) \ge 1 \rightarrow y_i(w^{T}x_{i}+b)\ge 1 - \xi_i$

松弛变量$\xi_i$: 允许犯的错误


### 如何惩罚


- 原始空间:

$$
argmin_{w,b,\xi}  \frac{1}{2} w^{Tw}+C \sum\limits_{i=1}^{n}\xi_{i}
$$
$$
s.t. y_i(w^{Tx_{i}+b)}\ge 1- \xi_{i,}\xi_{i}\ge 0
$$


$\xi_i$: 代价,是我们要最小化的函数
$\frac{1}{2} w^{T}w$: 正则项, 对分类器进行限制,使得模型的复杂度不要至于太高(还是最大化间隔)
$C$: 超参数, 一般是手工定义

- Soft margin的对偶形式
	- 只依赖于内积

$$
argmax_{a} \sum\limits_{i=1}^{n}a_{i}-\frac{1}{2}\sum\limits_{i=1}^n\sum\limits_{j=1}^{n}a_{i}a_{j}x_{i}^{T}x_{j} 
$$

$$
C\ge a_{i}\ge 0
$$
$$
\sum_{i=1}^{n}a_{i}y_{i}=0
$$

## 总结

#TODO 

# 非线性向量机

## 特征空间映射

- 把样本从原始空间映射到一个更高维空间, 是样本在这个特征空间线性可分
	- 如果原始空间是有限维, 那么一定存在一个高位特征空间使得样本可分

线性和非线性有时是紧密联系在一起--**通过内积**

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F31%2F2947abacfec8403f5cf8de5dd1c5cf34_20231031205833.png)


## Kernel trick


#TODO 
## 限制条件

#TODO 

必须存在特征映射, 才可以将非线性函数表示为特征空间中的内积

Mercer's condition

等价形式:


## 核支持向量机Kernel SVM

核函数: $K$
对偶形式: $argmax_{a}\sum\limits_{i=1}^{n}a_{i}- \frac{1}{2}\sum\limits_{i=1}^{n} $ 
边界分类:

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F31%2F9cf1c06420438fd5cc09b82c41c876cf_20231031210830.png)


## 线性核与非线性核

- 线性核: $K(x,y)=x^Ty$
- 非线性核 
	- RBF/高斯核$K(x,y)=exp(-\gamma ||x-y||^2)$
	- 多项式核$K(x,y)=(\gamma x^{T}y +c)^d$




## 如何选择超参数


用交叉验证在训练集上学习


# 多类支持向量机

## 1-vs-1

- 思路：转化为2类问题
- C个类别，通过两两组合构造$C_{c}^{2}$个分类器
- 一共$\frac{C(C-1)}{2}$个分类器，其中每个了类出现C-1此
	- 每个分类器$f_I$使用其二值输出，即$sign(f_i(x))$
- 对每个样本x，根据结果投票


## 1-vs-all

设计C个分类器，第i个分类器用类i作为正类，其它C-1个作为负类
每个发呢垒起使用其实值输出，即$f_i(x)$
每个分类器输出可以看作x属于类i的信心

最终输出选择信心最高的那个类


