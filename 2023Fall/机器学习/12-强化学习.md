---
aliases: 
tags:
  - 2023_Fall_机器学习
  - 课程
categories: 2023_Fall_机器学习
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 12-强化学习
date:  2023-12-25 08:12
modified:  2024-01-04 10:01
---

# 1. 起源

强化学习：不是数据驱动而是和环境的交互驱动，奖惩和试错(Trial and Error）

- 概念学习
	- 给定正例/反例，学习目标概念（如监督学习）
- 交互学习
	- 通过交互学习一个任务（如走出迷宫）
	- 系统/外部环境存在若干状态
	- 学习算法/动作影响状态的分布
	- Exploration： 尽快搜索解空间，有些解不合适，会犯错
	- Exploitation：尽可能不犯错

- 挑战
	- 环境、动作、反馈、模型
- 学习目标
	- 概念->决策
	- 优化目标：最大化长期奖励

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-17-41-bc622adb909bd8d2696d5cab5aa0bf73-20231225081740-29806b.png)

# 2. MDP模型

序列决策过程

## 2.1. 数学模型

- Markov Decision Process: **四元组表示**
	- S-set of states, 状态集合
	- A-set of actions, 动作集合
	- $\delta$- transition probability，状态转移概率
	- R–immediate reward function, 即时奖赏函数

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-19-37-5d92c25224ad160a811d22a3cf52a652-20231225081936-1289ce.png)

## 2.2. 状态和动作

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-21-19-f0ce8d2012988c44ac31cb1ad2f8cc57-20231225082118-abe2f0.png)

## 2.3. 奖赏

- $r(s_{0},a,s_{1})$： 从$s_0$到$s_1$采取动作a，获得的奖励
	- 确定性，已经知道
- R(s,a)=在状态s，采用动作a获得的奖赏
	- 不确定，是一个期望

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-23-54-dce0e72b7f3bf7bdb39ef52274ad3bfa-20231225082353-d06251.png)

## 2.4. 轨迹

- 在一次Episode中，所获得的经验或轨迹(trajectory)
	- $s_i$到$s_j$经过的实际路径
	- 确定性

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-24-56-9b99cd517463f299039b56db16c9e8e3-20231225082455-31a3c4.png)

## 2.5. 动作选择

- 目标
	- 最大化期望奖励
- 策略
	- 状态到动作的映射$\pi: S\rightarrow A$

### 2.5.1. 例: N-臂老虎机

- 单状态学习问题
	- 没有后续状态
	- 采取行动后只有reward

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-29-49-700c5dc9a30d6378693e734bf3835425-20231225082949-8f8fe7.png)

## 2.6. 返回函数

- 返回函数（面向多状态学习）
	- 将所有的即时奖励组成一个单一值
	- **返回函数通常是即时奖励值的线性组合**
- 问题
	- 轨迹中早期的奖励和晚期的奖励相比，谁更重要？
	- 系统是持续的，还是有终止状态？

- 有限窗口 Finite Horizon
	- $return=\sum\limits_{1\le i \le H} R(s_{i},a_{i})$
	- 链不知道有多长
- 无穷窗口
	- 有折扣 $return = \sum\limits_{i=0}^{\inf}\gamma^iR(s_{I},a_{i})$
		- $\gamma$一般小于1
	- 无折扣 $return=\frac{1}{N}\sum\limits_{i=0}^{N-1}R(s_i,a_i),N\rightarrow \inf$

## 2.7. 动作选择

- 学习目标：期望返回值
	- 最大化期望返回
- 学习的对象：策略
	- 状态到动作的映射
- 潜在公理：最优策略
	- 如果$\pi$是最有策略，则其从任一状态出发，均是最优的策略
	- **定理：必然存在一个确定性的最有策略**

# 3. 动态规划

给定一个完全已知的MDP模型

强化学习：只知道s,a的MDP模型

- <font color="#c00000">策略评估</font>
	- 给定一个策略$\pi$，评估其返回值
- <font color="#c00000">最优控制</font>
	- 寻找一个最优策略$\pi^{*}$
	- 从任一状态出发，其返回值都为最大

## 3.1. 值函数

- $V^{\pi}(s)$：从s状态出发，采用$\pi$策略，所获得的期望返回值
- $Q^{\pi}(s,a)$: 从s状态出发，采用a动作，继而采用$\pi$策略，所获得的期望返回值
	- 只在s状态使用a动作
- 最优值函数$V^{*}(s)$和$Q^{*}(s,a)$：采用最优策略$\pi^{*}$所获得的期望 返回值

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-48-06-5b8d964838becbe3dbfb2c906c629981-20231225084805-eaf037.png)

要求即使只自己决策一步，也要是最优的

## 3.2. 策略评估

- Bellman等式：有折扣的无限窗口
	- $V^{\pi}(s)=E_{s^{\prime} \sim \pi(s)}\left[R(s, \pi(s))+\gamma V^{\pi}\left(s^{\prime}\right)\right]$
	- 当前状态值函数+后续状态值函数
- 重写
	- $V^{\pi}(s)=E[R(s, \pi(s))]+\gamma \sum_{s^{\prime}} \delta\left(s, \pi(s), s^{\prime}\right) V^{\pi}\left(s^{\prime}\right)$

### 3.2.1. 例

确定的MDP模型

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F09-00-40-e6b052f0b7366323ab74aa6421be75e8-20231225090040-34cb54.png)

$\mathrm{V}^{\pi}\left(\mathrm{s}_{0}\right)=0+\gamma\left[\pi\left(\mathrm{s}_{0},+1\right) \mathrm{V}^{\pi}\left(\mathrm{s}_{1}\right)+\pi\left(\mathrm{s}_{0},-1\right) \mathrm{V}^{\pi}\left(\mathrm{s}_{3}\right)\right]$

可以写出四个方程

$$
\mathrm{V}^{\pi}\left(\mathrm{s}_{0}\right)=0+\left(\mathrm{V}^{\pi}\left(\mathrm{s}_{1}\right)+\mathrm{V}^{\pi}\left(\mathrm{s}_{3}\right)\right) / 4 
$$

$$
\begin{aligned} \mathrm{V}^{\pi}\left(\mathrm{s}_{0}\right) & =5 / 3 \\ \mathrm{~V}^{\pi}\left(\mathrm{s}_{1}\right) & =7 / 3 \\ \mathrm{~V}^{\pi}\left(\mathrm{s}_{2}\right) & =11 / 3 \\ \mathrm{~V}^{\pi}\left(\mathrm{s}_{3}\right) & =13 / 3\end{aligned}
$$

## 3.3. 最优控制

- 状态-动作对值函数(对任意确定策略$\pi$)

$$
Q^{\pi}(s, a)=E[R(s, a)]+\gamma \sum_{s^{\prime}} \delta\left(s, a, s^{\prime}\right)V^{\pi}\left(s^{\prime}\right)
$$

其中：

$$
 V^{\pi}(s)=Q^{\pi}(s, \pi(s, a))
$$

- 贪心策略
	- 选择使得值函数最大的行动
- $\epsilon-$贪心策略
	- $1-\epsilon$概率选择，$\pi(s)=argmax_{a}Q^{\pi}(s,a)$
	- 以$\epsilon$概率选择其他动作

## 3.4. 例

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F09-10-42-9abe9c4778621b809471c3fcf7c2b9ce-20231225091042-e3e92b.png)

$\pi$根据状态-动作值函数进行修改

## 3.5. 计算最优策略

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2024%2F01%2F04%2F10-59-32-19e94c50bb3c476ed850241c59e987aa-20240104105931-0d9c38.png)

# 4. 强化学习

无法通过bellman公式直接计算

## 4.1. Monte Carlo方法

学习率$\alpha$，

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F09-22-25-f5587bb8808bbc1c3f0f5bcf1cb87aeb-20231225092224-1fe21b.png)

### 4.1.1. 评价策略

- 学习$V^{\pi}(s)$
- 访问状态$s$，采取策略$\pi$，获得若干经验
- 在访问状态s 后，对所获得的返回，进行平均。

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2024%2F01%2F04%2F11-06-44-600d9439ddcab443d64090c35a7b6f80-20240104110643-de4235.png)

### 4.1.2. 最优控制

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F09-29-26-a92d9dd050b1e68d8768506fa1b45e98-20231225092925-74a712.png)

- MC策略迭代：使用MC方法对策略进行评估，计算值函数； 
- MC策略修正：根据值函数(或者状态-动作对值函数)，采 用贪心策略进行策略修正；

## 4.2. 时差学习

- Every-Visit MC
	- $V\left(s_{t}\right)=V\left(s_{t}\right)+\alpha\left[R_{t}-V\left(s_{t}\right)\right]$
	- 目标：在经过若干次平均后，得到真实的返回值
- 最简单的时间差分方法(Temporal Difference)
	- $V\left(s_{t}\right)=V\left(s_{t}\right)+\alpha\left[r_{t}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)\right]$
	- 目标：在每一次经验后，都对返回值进行估计

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F09-24-57-0e56d613bb406dfcac59b55e62bed14a-20231225092456-6884ac.png)

## 4.3. Bootstraps和Sampling

- Bootstraps
	- 前一个状态值可以用后一个状态估计
	- 通过一个估计值进行更新
	- 动态规划/时差学习中采用
	- 蒙特卡罗方法不采用
- 采样
	- 不通过估计值进行更新，而根据经验进行更新
	- 蒙特卡罗方法/时差学习中采用
	- 动态规划中不采用

## 4.4. 离策略(off-policy)和在策略(on-policy)

off：假设之后选择最优的action

- Q-learning
	- 不采用真实的next-a，而采用使得下一步Q最大的next-a
	- $Q(s, a) \leftarrow Q(s, a)+\mu\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right)$
- SARSA
	- $Q(s, a) \leftarrow Q(s, a)+\mu\left(r+\gamma Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right)$

## 4.5. N步TD预测

思路：当做TD回退式，可以看到“更远的未来”

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F09-35-06-aa524d815f8063b950992d400a9dda98-20231225093505-35eda7.png)

- Monte Carlo
- TD(0)
	- $\hat{R(t)}$
- TD(n)

## 4.6. N步回退学习

可以将多个R平均，形成新的R

## 4.7. TD(λ)

对历史上的s更新