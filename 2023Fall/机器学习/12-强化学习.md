---
aliases: 
tags: 
categories:
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 12-强化学习
date:  Monday,December 25th 2023
modified:  Monday,December 25th 2023
---

# 起源

强化学习：不是数据驱动而是和环境的交互驱动，奖惩和试错(Trial and Error）

- 概念学习
	- 给定正例/反例，学习目标概念（如监督学习）
- 交互学习
	- 通过交互学习一个任务（如走出迷宫）
	- 系统/外部环境存在若干状态
	- 学习算法/动作影响状态的分布
	- Exploration： 尽快搜索解空间，有些解不合适，会犯错
	- Exploitation：尽可能不犯错

- 挑战
	- 环境、动作、反馈、模型
- 学习目标
	- 概念->决策
	- 优化目标：最大化长期奖励

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-17-41-bc622adb909bd8d2696d5cab5aa0bf73-20231225081740-29806b.png)

# MDP模型

序列决策过程

## 数学模型

- Markov Decision Process:：四元组表示
	- S-set of states, 状态集合
	- A-set of actions, 动作集合
	- $\delta$- transition probability，状态转移概率
	- R–immediate reward function, 即时奖赏函数

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-19-37-5d92c25224ad160a811d22a3cf52a652-20231225081936-1289ce.png)

## 状态和动作

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-21-19-f0ce8d2012988c44ac31cb1ad2f8cc57-20231225082118-abe2f0.png)

## 奖赏

- $r(s_{0},a,s_{1})$： 从$s_0$到$s_a$采取动作a，获得的奖励
	- 确定性，已经知道
- R(s,a)=在状态s，采用动作a获得的奖赏
	- 不确定，是一个期望

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-23-54-dce0e72b7f3bf7bdb39ef52274ad3bfa-20231225082353-d06251.png)

## 轨迹

- 在一次Episode中，所获得的经验或轨迹(trajectory)
	- $s_i$到$s_j$经过的实际路径
	- 确定性

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-24-56-9b99cd517463f299039b56db16c9e8e3-20231225082455-31a3c4.png)

## 动作选择

- 目标
	- 最大化期望奖励（但状态化）
- 策略
	- 状态到动作的映射$\pi: S\rightarrow A$

### 例: N-臂老虎机

- 单状态学习问题
	- 没有后续状态
	- 采取行动后只有reward

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-29-49-700c5dc9a30d6378693e734bf3835425-20231225082949-8f8fe7.png)

## 返回函数

- 返回函数（面向多状态学习）
	- 将所有的即使奖励组成一个单一值
	- **返回函数通常是即时奖励值的线性组合**
- 问题
	- 轨迹中早期的奖励和晚期的奖励相比，谁更重要？
	- 系统是持续的，还是有终止状态？

- 有限窗口 Finite Horizon
	- $return=\sum\limits_{1\le i \le H} R(s_{i},a_{i})$
	- 链不知道有多长
- 无穷窗口
	- 有折扣 $return = \sum\limits_{i=0}^{\inf}\gamma^iR(s_{I},a_{i})$
		- $\gamma$一般小于1
	- 无折扣 $return=\frac{1}{N}\sum\limits_{i=0}^{N-1}R(s_i,a_i),N\rightarrow \inf$

## 动作选择

- 学习目标：期望返回值
	- 最大化期望返回
- 学习的对象：策略
	- 状态到动作的映射
- 潜在公理：最优策略
	- 如果$\pi$是最有策略，则其从任一状态出发，均是最优的策略
	- **定理：必然存在一个确定性的最有策略**

# 动态规划

给定一个完全已知的MDP模型

强化学习：只知道s,a的MDP模型


- 策略评估
	- 给定一个策略$\pi$，评估其返回值
- 最优控制
	- 寻找一个最优策略$\pi^{*}$
	- 从任一状态出发，其返回值都为最大

## 值函数

- $V^{\pi}(s)$：从s状态出发，采用$\pi$策略，所获得的期望返回值
- $Q^{\pi}(s,a)$: 从s状态出发，采用a动作，继而采用$\pi$策略，所获得的期望返回值
	- 只在s状态使用a动作
- 最优值函数$V^{*}(s)$和$Q^{*}(s,a)$：采用最优策略$\pi^{*}$所获得的期望 返回值

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-48-06-5b8d964838becbe3dbfb2c906c629981-20231225084805-eaf037.png)

要求即使只自己决策一步，也要是最优的

## 策略评估

- Bellman等式：有折扣的无限窗口
	- $V^{\pi}(s)=E_{s^{\prime} \sim \pi(s)}\left[R(s, \pi(s))+\gamma V^{\pi}\left(s^{\prime}\right)\right]$
	- 当前状态值函数+后续状态值函数
- 展开
	- $V^{\pi}(s)=E[R(s, \pi(s))]+\gamma \sum_{s^{\prime}} \delta\left(s, \pi(s), s^{\prime}\right) V^{\pi}\left(s^{\prime}\right)$

### 例：

q

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F09-00-40-e6b052f0b7366323ab74aa6421be75e8-20231225090040-34cb54.png)
