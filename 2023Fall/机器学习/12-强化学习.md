---
aliases: 
tags: 
categories:
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 12-强化学习
date:  Monday,December 25th 2023
modified:  Monday,December 25th 2023
---

# 起源

强化学习：不是数据驱动而是和环境的交互驱动，奖惩和试错(Trial and Error）

- 概念学习
	- 给定正例/反例，学习目标概念（如监督学习）
- 交互学习
	- 通过交互学习一个任务（如走出迷宫）
	- 系统/外部环境存在若干状态
	- 学习算法/动作影响状态的分布
	- Exploration： 尽快搜索解空间，有些解不合适，会犯错
	- Exploitation：尽可能不犯错

- 挑战
	- 环境、动作、反馈、模型
- 学习目标
	- 概念->决策
	- 优化目标：最大化长期奖励

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-17-41-bc622adb909bd8d2696d5cab5aa0bf73-20231225081740-29806b.png)

# MDP模型

## 数学模型

- Markov Decision Process:：四元组表示
	- S-set of states, 状态集合
	- A-set of actions, 动作集合
	- $\delta$- transition probability，状态转移概率
	- R–immediate reward function, 即时奖赏函数

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-19-37-5d92c25224ad160a811d22a3cf52a652-20231225081936-1289ce.png)

## 状态和动作

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-21-19-f0ce8d2012988c44ac31cb1ad2f8cc57-20231225082118-abe2f0.png)

## 奖赏

- $r(s_{0},a,s_{1})$： 从$s_0$到$s_a$采取动作a，获得的奖励
	- 确定性，已经知道
- R(s,a)=在状态s，采用动作a获得的奖赏
	- 不确定，是一个期望

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-23-54-dce0e72b7f3bf7bdb39ef52274ad3bfa-20231225082353-d06251.png)

## 轨迹

- 在一次Episode中，所获得的经验或轨迹(trajectory)
	- $s_i$到$s_j$经过的实际路径
	- 确定性

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-24-56-9b99cd517463f299039b56db16c9e8e3-20231225082455-31a3c4.png)


## 动作选择

- 目标
	- 最大化期望奖励（但状态化）
- 策略
	- 状态到动作的映射$\pi: S\rightarrow A$

### 例: N-臂老虎机

- 单状态学习问题
	- 没有后续状态
	- 采取行动后只有reward

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F25%2F08-29-49-700c5dc9a30d6378693e734bf3835425-20231225082949-8f8fe7.png)

