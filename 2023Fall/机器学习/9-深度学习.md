---
aliases: 
tags:
  - 2023_Fall_机器学习
  - 课程
categories: 2023_Fall_机器学习
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 9-深度学习
date:  Tuesday,November 21st 2023
modified:  Tuesday,December 26th 2023
---

深度学习网络：学习良好的非线性结构

- 从模式识别的角度看
	- 手工特征：固定、难以设计、人工、低层特征、需要专业知识、分离的
	- 深度学习：可学习的、黑盒的、任务相关、学习层次表示和高级特征、端到端的

# 1. 基础知识

## 1.1. 卷积 Convolution

从图像处理中发展出来

- 特征提取
- 超参数：depth,stride,zero-padding
	- depth: filter的数量
	- zero-padding: 输入特征图的周围填充一圈零值像素。填充的目的是为了保持输入和输出特征图的尺寸一致或者控制尺寸的变化。
	- stride: 指卷积核在输入特征图上滑动的步长
- 过滤器: 与输入数据的局部区域进行逐元素相乘，并将结果相加，从而提取输入数据中的特征
	- 也被称为卷积核（convolutional kernel）或特征检测器（feature detector），它是一个小的矩阵或张量，用于在输入数据上进行卷积操作。

![msedge_P45nvi3Q2K.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F11%2F21%2F20-35-48-af72257a870dc0f92d74f9f6ce5ff219-msedge_P45nvi3Q2K-fdaab4.png)

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/convolutional-networks/)

## 1.2. 池化Pooling/下采样Subsampling

- 减少参数
- 避免过拟合
- 扩大感受域

- 常用方法
	- Max Pooling
	- Average Pooling

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F11%2F21%2F20-40-50-fccd7cb28f4bfb97f7dc72a72bb1f89b-20231121204049-4abb64.png)

max pooling:  
![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F11%2F21%2F20-40-35-e23daf90d7b5dab4cbdac82874b804c9-20231121204034-bd3ed1.png)

## 1.3. 全连接层

MLP

FC Layer：**全局特征提取**  
Softmax Layer：分类层

## 1.4. Dropout

参数过多，但数据又不够时，容易过拟合

每个神经元以固定概率p被弃用

## 1.5. Batch Normalization

为了让神经网络更深，减少网络内部方差偏移或梯度扩散

对每一层神经网络的输出做归一化

# 2. 深度学习技巧

1. 数据增广 Data Agumentation
2. 预处理 Pre-Processing
3. 初始化 Initilization
4. 过滤器 Filter
	- Batch size
	- Filter、stride
5. 池化大小 Pooling size
6. 学习率 Learning rate
	- 和使用的优化器有关系

