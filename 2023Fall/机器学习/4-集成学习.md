---
aliases: 
tags: 
categories:
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 4-集成学习
date:  Tuesday,October 17th 2023
modified:  Tuesday,October 17th 2023
---

与树学习有很多联系

# 原理

三个臭皮匠，顶个诸葛亮

- 是一个预测模型的**元方法**
	- 不是一个具体的学习算法
- 思想：单个学习器无法取得比较好的效果，可以使用多个不同的学习器进行集成

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F2cfa45984d8a968969c47daa409d232a_20231017183550.png)

## 特点（分类）

1. 多个分类器集成在一起，以提高分类准确率
2. 由训练数据构筑基分类器，然后根据预测结果进行投票
3. 集成学习本身不是一种分类器，而是分类器结合方法
4. <font color="#ff0000">通常集成分类器性能回优于单个分类器</font>（并不是绝对的）

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2Fef60d04354160077f30d72479e1d8584_20231017184612.png)

例子：以多数投票法为结合方法，每个二分类器分类精度为$p$，集成$T$个二分类器的分类精度：

$$
\sum\limits_{k=\frac{T}{2}+1}^{T}  C_{T}^{k}p^{k}（1-p）^{T-k}
$$

$p\gt 0.5$ T → ∞，上式→ 1

准确性和多样性之间的矛盾

## Bias-Variance tradeoff

- Bias：学习结果的期望与真实规律的差距
	- $Bias = E[\hat{f}](x)-f(x)$
- Variance：学习结果自身的不稳定性
	- $Variance = E[({\hat{f}}(x)- E[\hat{f}(x)])^2]$
- Total Error以均方误差为例
	- 模型的错误由三部分构成
	- $Err(x)=Bias^{2}+ Variance +Random Error$

例子：深度学习模型更复杂，比起KNN、SVM更能拟合训练数据，但Random Error也随之增加。

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F1dc0fbba69d007e0ac5b0fc907cc958c_20231017190433.png)

## 核心问题

- 序列集成（基学习器）法
	- 利用基学习器之间的**依赖关系**，一次生成
	- 减小偏差bias
- 并行集成（基学习器）法
	- 利用基学习器之间的**独立关系**，并行学习
	- 减小方差variance

## 结合策略

- 平均法（回归）
	- 简单平均
	- 加权平均
- 投票法（分类）
	- 绝对多数
	- 相对多俗
	- 加权投票

## 多样性策略（学习基学习器）

- 数据层面
	- 输入样本扰动，构建基学习器
	- 输出样本扰动
- 属性层面
	- 随机选择部分属性
- 参数层面
	- 算法模型参数的扰动

# Bagging和随机森林

- Bagging (Bootstrap aggregating)基本原理
	- 自举/自助法
	- **有放回随机采样方法**。统计上的目的是得到统计量分布以及置信区间

通过有放回采样，拆分样本空间，是的训练出的基学习器存在差异

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F58639928155fb5dcef03931d70f2c59f_20231017191941.png)

