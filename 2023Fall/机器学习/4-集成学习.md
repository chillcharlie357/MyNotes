---
aliases: 
tags: 
categories:
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 4-集成学习
date:  Tuesday,October 17th 2023
modified:  Tuesday,October 17th 2023
---

与树学习有很多联系

# 原理

三个臭皮匠，顶个诸葛亮

- 是一个预测模型的**元方法**
	- 不是一个具体的学习算法
- 思想：单个学习器无法取得比较好的效果，可以使用多个不同的学习器进行集成

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F2cfa45984d8a968969c47daa409d232a_20231017183550.png)

## 特点（分类）

1. 多个分类器集成在一起，以提高分类准确率
2. 由训练数据构筑基分类器，然后根据预测结果进行投票
3. 集成学习本身不是一种分类器，而是分类器结合方法
4. <font color="#ff0000">通常集成分类器性能回优于单个分类器</font>（并不是绝对的）

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2Fef60d04354160077f30d72479e1d8584_20231017184612.png)


集成$T$个二分类器的分类精度：
$$
\sum\limits_{k=\frac{T}{2}+1}^{T}  C_{T}^{k}p^{k}（1-p）^{T-k}
$$

$p\gt 0.5$  T → ∞，上式→ 1

## Bias-Variance tradeoff

- Bias：学习结果的期望与真实规律的差距
	- $Bias = E[\hat{f}](x)-f(x)$
- Variance：学习结果自身的不稳定性
	- $Variance = E[({\hat{f}}(x)- E[\hat{f}(x)])^2]$
- Total Error以均方误差为例
	- $Err(x)=Bias^{2}+ Variance +Random Error$
# Bagging和随机森林

