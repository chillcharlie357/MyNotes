---
aliases: 
tags:
  - 2023_Fall_机器学习
  - 课程
categories: 2023_Fall_机器学习
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 4-集成学习
date:  Tuesday,October 17th 2023
modified:  Tuesday,October 17th 2023
---

与树学习有很多联系

# 1. 原理

三个臭皮匠，顶个诸葛亮

- 是一个预测模型的**元方法**
	- 不是一个具体的学习算法
- 思想：单个学习器无法取得比较好的效果，可以使用多个不同的学习器进行集成

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F2cfa45984d8a968969c47daa409d232a_20231017183550.png)

## 1.1. 特点（分类）

1. 多个分类器集成在一起，以提高分类准确率
2. 由训练数据构筑基分类器，然后根据预测结果进行投票
3. 集成学习本身不是一种分类器，而是分类器结合方法
4. <font color="#ff0000">通常集成分类器性能回优于单个分类器</font>（并不是绝对的）

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2Fef60d04354160077f30d72479e1d8584_20231017184612.png)

例子：以多数投票法为结合方法，每个二分类器分类精度为$p$，集成$T$个二分类器的分类精度：

$$
\sum\limits_{k=\frac{T}{2}+1}^{T}  C_{T}^{k}p^{k}（1-p）^{T-k}
$$

$p\gt 0.5$ T → ∞，上式→ 1

准确性和多样性之间的矛盾

## 1.2. Bias-Variance tradeoff

- Bias：学习结果的期望与真实规律的差距
	- $Bias = E[\hat{f}](x)-f(x)$
- Variance：学习结果自身的不稳定性
	- $Variance = E[({\hat{f}}(x)- E[\hat{f}(x)])^2]$
- Total Error以均方误差为例
	- 模型的错误由三部分构成
	- $Err(x)=Bias^{2}+ Variance +Random Error$

例子：深度学习模型更复杂，比起KNN、SVM更能拟合训练数据，但Random Error也随之增加。

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F1dc0fbba69d007e0ac5b0fc907cc958c_20231017190433.png)

## 1.3. 核心问题

- 序列集成（基学习器）法
	- 利用基学习器之间的**依赖关系**，一次生成
	- 减小偏差bias
- 并行集成（基学习器）法
	- 利用基学习器之间的**独立关系**，并行学习
	- 减小方差variance

## 1.4. 结合策略

- 平均法（回归）
	- 简单平均
	- 加权平均
- 投票法（分类）
	- 绝对多数
	- 相对多俗
	- 加权投票

## 1.5. 多样性策略（学习基学习器）

- 数据层面
	- 输入样本扰动，构建基学习器
	- 输出样本扰动
- 属性层面
	- 随机选择部分属性
- 参数层面
	- 算法模型参数的扰动

# 2. Bagging和随机森林

- Bagging (Bootstrap aggregating)基本原理
	- 自举/自助法，<font color="#ff0000">数据层面的扰动</font>
	- **有放回随机采样方法**。统计上的目的是得到统计量分布以及置信区间

通过有放回采样，拆分样本空间，是的训练出的基学习器存在差异

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F58639928155fb5dcef03931d70f2c59f_20231017191941.png)

## 2.1. 优点

- 并行式学习，降低分类器方差，改善泛化
	- 其性能依赖于基分类器的稳定性。如果基分类器稳定，则其误差主要由基分类器的bias决定
	- 由于采样概率相同bagging方法并不侧重于任何实例
	- 可以并行化处理，提高效率

## 2.2. 缺点

- 当基学习器具有高Bias，集成之后也具有高Bias
- 集成之后会缺乏可解释性
- 依赖数据集，计算可能会比较昂贵

## 2.3. 代表性算法：随机森林

N代表训练样例的个数，M表示特征数目；输入特征数目m，由于缺点决策树上一个节点的决策结果；其中m应远小于M。

1. 从N个训练用例中以有放回抽样的方式，取样N次，形成一个训练集即**bagging取样**，并用未用到的样例做预测，估计其误差
2. 对于每一个节点，随机选取m个特征（通常为M的均方根$log_2M$），根据这m个特征，计算器最佳的分裂方式
3. 每棵树都会完整成长而不会剪枝，这又可能在建成一颗正常树状分类器后被采用
4. 重复以上过程充分多次，以产生足够多的随机树

### 2.3.1. 特点

- **差异性**：每棵树不同，使用的特征也不同
- **缓解维度灾难**：因为每棵树都没有使用全部的特征，特征空间被缩小了，计算代价减小
- **可并行化**：因为每棵树都使用不同特征、数据，可以用并行化方法加速
- **训练-测试划分**：训练和测试的划分不是必须的，因为构建每棵树时，**因为总有30%的数据没有采样**
	- 30%可以算出来
- **稳定性**：通过多数投票或者平均，结果较为稳定

# 3. Boosting和AdaBoost

## 3.1. 提示Boosting

- PAC学习理论/概率近似正确理论
	1. 强可学习(Strongly learnable)：在PAC框架中，一个概念/类，如果存在一个多项式的学习算法能够学习他，并且正确率高，那么称这个概念为强可学习
	2. 强可学习和若可学习时等价的；
	3. 一个概念时强可学习的充要条件时这个概念是弱可学习的
	4. 通过提升方法可以将弱学习器转化为强学习器

每一次都更关注上一轮预测错误的样本，是序列集成的方法

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F45035fbc8a1a37530b9c2141ed0365e4_20231017200027.png)

## 3.2. Adaptive Boost

- 思想：从弱学习算法开始，改变训练数据的概率分布（权值分布），反复学习，得到一系列弱分类器然后进行组合，构成一个强分类器
- 策略：
	- 权值分布：提高哪些被前一轮弱分类器错误分类样本的权值，降低哪些被正确分类样本的权值

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2Ff8ef14875291145f0f89c4149cc4a67b_20231017200419.png)

以分类问题为例：

- 分类器的误差率和权重系数：

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2Fcba18bce4ccf8d2c7360323b336ae205_20231017200436.png)

- 样本的权重和集合策略

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2Fd585a658536262f2b2e764a18830b46b_20231017200530.png)

$Z_k$用于归一化

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2Fcdb92b140df0e1c9a152b5e29ebb5087_20231017200557.png)

sign表决加权

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F8c5adbbf6db3125e49f19483f77468b3_20231017200900.png)

## 3.3. AdaBoost算法解释

- 加法模型（不同模型加权求和）

- 目标函数：

$$
f(X)=\sum\limits_{m=1}^{M}\beta_mb(x_{i};\gamma_m)
$$

*优化复杂*

### 3.3.1. 前向分布算法

- **前向分布算法**：学习目标函数是加法模型，如果能从前往后，**每一步只学习一个基函数及其系数**，逐步逼近总目标函数，可以降低优化复杂度。

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F014198927052dcd1982d63d5aec4d141_20231017201743.png)

前向分布算法与AdaBoost

# Boosting和GBDT、XGBoost

## Boosting Tree

- Boosting方法主要采用加法模型，即基函数的线性组合，与前向分布算法
- 以决策树为基函数的提升方法为决策树boosting tree

在回归树中：

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F9c0037a851ca49dac79d9de475422dc0_20231017203613.png)

### 计算损失

- 使用平方误差损失函数

$$
L(y,f_{m-1}(x)+h_m(x))=(y-f_{m-1}(x)-h_{m(x))^{2}=}(r - h_m(x))^2
$$

$r=(y-f_{m-1}(x))$，r是当前模型的残差

推广到一般的损失：第m轮的第i个样本的负梯度：  
负梯度作为残差近似值

## 梯度提升树GBDT

《The Elements of Statistical Learning》

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F4c899b306bc54b3935700cc87ca2d4f0_20231017204757.png)

## XGBoost

- 详细教程
	- [Introduction to Boosted Trees — xgboost 2.0.0 documentation](https://xgboost.readthedocs.io/en/stable/tutorials/model.html)



# Stacking学习


对初始数据集训练初始学习器，生成一个新的数据集（元学习器）。然后用新的数据集训练次级学习器。

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F10%2F17%2F825c4e9ba08e5415203cec7183371ab3_20231017210037.png)
