---
aliases: 
tags:
  - 2023_Fall_机器学习
  - 课程
categories: 2023_Fall_机器学习
sticky:
thumbnail:
cover: 
excerpt: false
mathjax: true
comment: true
title: 11-维度约简
date:  2023-12-12 18:12
modified:  2024-01-04 10:01
---

# 1. 特征选择和降维

- 维度约简
	- 特征选择
	- 特征变化/诱导
- 目的
	- 降维，减少over-fitting风险
	- 增加解释性
	- 去除冗余特征

## 1.1. 特征选则

- 搜索问题
	- N个原始特征，$2^{N}-1$非空特征空间，搜索最优的特征子集
- 搜索起点和方向
	- 前向（起点为空集）
	- 后向（起点为全集）
	- 双向

# 2. 线性判别分析 LDA

- 给定标注了类别的高维数据集，投影到高维的超平面，使得样本点按照类别尽可能最大区分开

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F12%2F12%2F19-00-19-ce756b9b3984654e5776c026b3eff5c2-20231212190017-65425d.png)

## 2.1. 算法流程

1. 计算每个类别的均值$\mu_{i}$, 全局样本均值$\mu$  
2. 计算类内散度矩阵$S_W$，类间散度矩阵$S_B$  
3. 对矩阵$S_{W}^{-1}S_B$做特征值分解  
4. 取最大的几个特征值所对应的特征向量  
5. 计算投影矩阵

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2024%2F01%2F04%2F09-13-00-c9e519e8f613c18c498a4c9246fefdbf-20240104091259-16d2c2.png)

# 主成分分析 PCA

找到数据中的主要成分，并为之表征数据。

## 主成分特点

1. 最大可分性：样本点在第一主成分上的投影离散程度大于其在第二主成分上的离散程度
2. 最近可重构性：样本点到第一主成分线的平均距离都要小于其到第二主成分线的距离

- 最大可分性理论的目标函数
	- 最大化样本点在主成分上投影的方差

## 算法流程

1. 样本去中心化
	- 每一行减去这一行的均值
2. 计算样本的协方差矩阵
3. 对**协方差矩阵**做特征值分解
4. 取最大的数个特征值所对应的特征向量
5. 计算投影矩阵

![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2024%2F01%2F04%2F21-51-22-69829b78b8d39177ee63354216f2a6cb-20240104215121-024df3.png)

# ICA

ICA是一个解混的过程，认为矩阵是由多个矩阵线性组合构成。认为数据源都不符合高斯分布，因为要独立，然后混合的数据要符合高斯分布。

假设一个信号S，经过混和矩阵变换为了X=AS。

对X求解混矩阵W，Y=WX